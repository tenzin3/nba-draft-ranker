{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e1b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load data\n",
    "# ----------------------------\n",
    "root_dir = Path.cwd().parent.parent\n",
    "dataset_path = root_dir / \"outputs\" / \"college_stats.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ee8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. Simple ranking model\n",
    "# ----------------------------\n",
    "class RankMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, D]\n",
    "        return self.net(x).squeeze(-1)  # [N]\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Listwise losses\n",
    "# ----------------------------\n",
    "\n",
    "def listnet_loss(scores, labels):\n",
    "    \"\"\"\n",
    "    ListNet top-1 cross entropy.\n",
    "    scores: [N] model scores (higher means better)\n",
    "    labels: [N] OVERALL_PICK (lower is better in reality)\n",
    "    We convert labels to relevance by rel = -labels.\n",
    "    \"\"\"\n",
    "    rel = -labels  # larger rel = better\n",
    "    P_y = F.softmax(rel, dim=0)\n",
    "    P_s = F.softmax(scores, dim=0)\n",
    "    loss = -torch.sum(P_y * torch.log(P_s + 1e-12))\n",
    "    return loss\n",
    "\n",
    "def listmle_loss(scores, labels):\n",
    "    \"\"\"\n",
    "    ListMLE loss.\n",
    "    scores: [N]\n",
    "    labels: [N] OVERALL_PICK (lower = better)\n",
    "    We sort items by true ranking (ascending OVERALL_PICK).\n",
    "    \"\"\"\n",
    "    # sort by true rank: best (smallest pick) first\n",
    "    _, idx = torch.sort(labels, descending=False)\n",
    "    s_sorted = scores[idx]\n",
    "\n",
    "    # log-sum-exp over suffixes:\n",
    "    # denominator for position i is sum_{j>=i} exp(s_j)\n",
    "    log_cumsumexp = torch.logcumsumexp(s_sorted.flip(0), dim=0).flip(0)\n",
    "\n",
    "    # log-likelihood: sum_i [s_i - log(sum_{j>=i} exp(s_j))]\n",
    "    log_likelihood = torch.sum(s_sorted - log_cumsumexp)\n",
    "    return -log_likelihood  # negate to get loss\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Evaluation: pairwise ranking accuracy\n",
    "# ----------------------------\n",
    "def pairwise_accuracy(scores, labels):\n",
    "    \"\"\"\n",
    "    Pairwise accuracy within one list.\n",
    "    True order: lower OVERALL_PICK is better.\n",
    "    \"\"\"\n",
    "    scores = scores.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    n = len(labels)\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            total += 1\n",
    "            true_better = labels[i] < labels[j]  # True if i should rank ahead of j\n",
    "            pred_better = scores[i] > scores[j]  # True if model scores i > j\n",
    "            if (true_better and pred_better) or ((not true_better) and (not pred_better)):\n",
    "                correct += 1\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def evaluate_model(model, groups):\n",
    "    model.eval()\n",
    "    total_correct = 0.0\n",
    "    total_pairs = 0\n",
    "    with torch.no_grad():\n",
    "        for season, X, y in groups:\n",
    "            s = model(X)\n",
    "            n = len(y)\n",
    "            if n < 2:\n",
    "                continue\n",
    "            n_pairs = n * (n - 1) // 2\n",
    "            acc = pairwise_accuracy(s, y)\n",
    "            total_correct += acc * n_pairs\n",
    "            total_pairs += n_pairs\n",
    "    return total_correct / total_pairs if total_pairs > 0 else 0.0\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Training loop helper\n",
    "# ----------------------------\n",
    "def train_listwise(model, groups, loss_fn, n_epochs=200, lr=1e-3, name=\"model\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for season, X, y in groups:\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(X)\n",
    "            loss = loss_fn(scores, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / max(len(groups), 1)\n",
    "\n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            print(f\"[{name}] Epoch {epoch:3d} | train loss = {avg_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e36afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. K-fold cross-validation\n",
    "# ----------------------------\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def prepare_kfold_folds(df, feature_cols, k_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare K-fold season-wise data for listwise ranking.\n",
    "\n",
    "    Returns a list of folds, where each fold is a dict:\n",
    "      {\n",
    "        \"fold_id\": int,\n",
    "        \"train_seasons\": [...],\n",
    "        \"test_seasons\":  [...],\n",
    "        \"train_groups\":  [(season, X, y), ...],\n",
    "        \"test_groups\":   [(season, X, y), ...],\n",
    "      }\n",
    "\n",
    "    Each fold has its own scaling (mean/std) computed from that fold's TRAIN seasons only.\n",
    "    \"\"\"\n",
    "    all_seasons = sorted(df[\"SEASON\"].unique())\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    folds = []\n",
    "\n",
    "    for fold_id, (train_idx, test_idx) in enumerate(kf.split(all_seasons), start=1):\n",
    "        train_seasons = [all_seasons[i] for i in train_idx]\n",
    "        test_seasons  = [all_seasons[i] for i in test_idx]\n",
    "\n",
    "        df_train = df[df[\"SEASON\"].isin(train_seasons)].copy()\n",
    "        df_test  = df[df[\"SEASON\"].isin(test_seasons)].copy()\n",
    "\n",
    "        # ---- scaling: fit ONLY on this fold's training data ----\n",
    "        train_feats = df_train[feature_cols]\n",
    "        feat_mean = train_feats.mean()\n",
    "        feat_std  = train_feats.std().replace(0, 1.0)\n",
    "\n",
    "        def make_groups(df_subset, seasons_subset):\n",
    "            groups = []\n",
    "            for season in seasons_subset:\n",
    "                g = df_subset[df_subset[\"SEASON\"] == season].copy()\n",
    "                if g.empty:\n",
    "                    continue\n",
    "\n",
    "                g = g.sort_values(\"OVERALL_PICK\")  # lower pick = better\n",
    "                g_scaled = (g[feature_cols] - feat_mean) / feat_std\n",
    "\n",
    "                X = torch.tensor(g_scaled.values, dtype=torch.float32)\n",
    "                y = torch.tensor(g[\"OVERALL_PICK\"].values, dtype=torch.float32)\n",
    "                groups.append((season, X, y))\n",
    "            return groups\n",
    "\n",
    "        train_groups = make_groups(df_train, train_seasons)\n",
    "        test_groups  = make_groups(df_test,  test_seasons)\n",
    "\n",
    "        folds.append({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"train_seasons\": train_seasons,\n",
    "            \"test_seasons\":  test_seasons,\n",
    "            \"train_groups\":  train_groups,\n",
    "            \"test_groups\":   test_groups,\n",
    "        })\n",
    "\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35b1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"player_name\", \"OVERALL_PICK\", \"SEASON\"]\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "folds = prepare_kfold_folds(\n",
    "    df=df,\n",
    "    feature_cols=feature_cols,\n",
    "    k_folds=5,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3073ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_for_loss(folds, loss_fn, model_name, n_epochs=200, hidden_dim=64, lr=1e-3):\n",
    "    train_accs = []\n",
    "    test_accs  = []\n",
    "\n",
    "    for fold in folds:\n",
    "        fold_id = fold[\"fold_id\"]\n",
    "        train_groups = fold[\"train_groups\"]\n",
    "        test_groups  = fold[\"test_groups\"]\n",
    "\n",
    "        print(f\"\\n===== {model_name} | Fold {fold_id} =====\")\n",
    "        print(\"Train seasons:\", fold[\"train_seasons\"])\n",
    "        print(\"Test  seasons:\", fold[\"test_seasons\"])\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "        model = RankMLP(input_dim=len(feature_cols), hidden_dim=hidden_dim)\n",
    "\n",
    "        model = train_listwise(\n",
    "            model,\n",
    "            train_groups,\n",
    "            loss_fn=loss_fn,\n",
    "            n_epochs=n_epochs,\n",
    "            lr=lr,\n",
    "            name=f\"{model_name} Fold {fold_id}\"\n",
    "        )\n",
    "\n",
    "        train_acc = evaluate_model(model, train_groups)\n",
    "        test_acc  = evaluate_model(model, test_groups)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f\"[{model_name} Fold {fold_id}] \"\n",
    "              f\"Train pairwise acc = {train_acc:.3f} | \"\n",
    "              f\"Test pairwise acc = {test_acc:.3f}\")\n",
    "\n",
    "    print(f\"\\n=== {model_name} {len(folds)}-fold CV (season-wise) ===\")\n",
    "    for i, (tr, te) in enumerate(zip(train_accs, test_accs), start=1):\n",
    "        print(f\"Fold {i}: train = {tr:.3f}, test = {te:.3f}\")\n",
    "    print(f\"Mean train acc: {np.mean(train_accs):.3f}\")\n",
    "    print(f\"Mean  test acc: {np.mean(test_accs):.3f}\")\n",
    "\n",
    "    return train_accs, test_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e1eb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ListNet | Fold 1 =====\n",
      "Train seasons: [np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2010), np.int64(2011), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2000), np.int64(2009), np.int64(2012), np.int64(2017), np.int64(2024)]\n",
      "[ListNet Fold 1] Epoch   1 | train loss = 3.7765\n",
      "[ListNet Fold 1] Epoch  20 | train loss = 2.5079\n",
      "[ListNet Fold 1] Epoch  40 | train loss = 2.2566\n",
      "[ListNet Fold 1] Epoch  60 | train loss = 2.0607\n",
      "[ListNet Fold 1] Epoch  80 | train loss = 1.9108\n",
      "[ListNet Fold 1] Epoch 100 | train loss = 1.7987\n",
      "[ListNet Fold 1] Epoch 120 | train loss = 1.7069\n",
      "[ListNet Fold 1] Epoch 140 | train loss = 1.6306\n",
      "[ListNet Fold 1] Epoch 160 | train loss = 1.5624\n",
      "[ListNet Fold 1] Epoch 180 | train loss = 1.5000\n",
      "[ListNet Fold 1] Epoch 200 | train loss = 1.4438\n",
      "[ListNet Fold 1] Train pairwise acc = 0.697 | Test pairwise acc = 0.688\n",
      "\n",
      "===== ListNet | Fold 2 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2001), np.int64(2006), np.int64(2010), np.int64(2014), np.int64(2023)]\n",
      "[ListNet Fold 2] Epoch   1 | train loss = 3.7557\n",
      "[ListNet Fold 2] Epoch  20 | train loss = 2.2418\n",
      "[ListNet Fold 2] Epoch  40 | train loss = 2.0102\n",
      "[ListNet Fold 2] Epoch  60 | train loss = 1.8397\n",
      "[ListNet Fold 2] Epoch  80 | train loss = 1.7105\n",
      "[ListNet Fold 2] Epoch 100 | train loss = 1.6058\n",
      "[ListNet Fold 2] Epoch 120 | train loss = 1.5150\n",
      "[ListNet Fold 2] Epoch 140 | train loss = 1.4424\n",
      "[ListNet Fold 2] Epoch 160 | train loss = 1.3788\n",
      "[ListNet Fold 2] Epoch 180 | train loss = 1.3276\n",
      "[ListNet Fold 2] Epoch 200 | train loss = 1.2848\n",
      "[ListNet Fold 2] Train pairwise acc = 0.692 | Test pairwise acc = 0.647\n",
      "\n",
      "===== ListNet | Fold 3 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2014), np.int64(2015), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2013), np.int64(2016)]\n",
      "[ListNet Fold 3] Epoch   1 | train loss = 3.8097\n",
      "[ListNet Fold 3] Epoch  20 | train loss = 2.4647\n",
      "[ListNet Fold 3] Epoch  40 | train loss = 2.1812\n",
      "[ListNet Fold 3] Epoch  60 | train loss = 1.9629\n",
      "[ListNet Fold 3] Epoch  80 | train loss = 1.7924\n",
      "[ListNet Fold 3] Epoch 100 | train loss = 1.6587\n",
      "[ListNet Fold 3] Epoch 120 | train loss = 1.5579\n",
      "[ListNet Fold 3] Epoch 140 | train loss = 1.4794\n",
      "[ListNet Fold 3] Epoch 160 | train loss = 1.4150\n",
      "[ListNet Fold 3] Epoch 180 | train loss = 1.3575\n",
      "[ListNet Fold 3] Epoch 200 | train loss = 1.3048\n",
      "[ListNet Fold 3] Train pairwise acc = 0.695 | Test pairwise acc = 0.668\n",
      "\n",
      "===== ListNet | Fold 4 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2020), np.int64(2023), np.int64(2024)]\n",
      "Test  seasons: [np.int64(2018), np.int64(2019), np.int64(2021), np.int64(2022), np.int64(2025)]\n",
      "[ListNet Fold 4] Epoch   1 | train loss = 3.7906\n",
      "[ListNet Fold 4] Epoch  20 | train loss = 2.5592\n",
      "[ListNet Fold 4] Epoch  40 | train loss = 2.3019\n",
      "[ListNet Fold 4] Epoch  60 | train loss = 2.0841\n",
      "[ListNet Fold 4] Epoch  80 | train loss = 1.9049\n",
      "[ListNet Fold 4] Epoch 100 | train loss = 1.7569\n",
      "[ListNet Fold 4] Epoch 120 | train loss = 1.6447\n",
      "[ListNet Fold 4] Epoch 140 | train loss = 1.5554\n",
      "[ListNet Fold 4] Epoch 160 | train loss = 1.4816\n",
      "[ListNet Fold 4] Epoch 180 | train loss = 1.4222\n",
      "[ListNet Fold 4] Epoch 200 | train loss = 1.3707\n",
      "[ListNet Fold 4] Train pairwise acc = 0.698 | Test pairwise acc = 0.686\n",
      "\n",
      "===== ListNet | Fold 5 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2009), np.int64(2010), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2007), np.int64(2008), np.int64(2011), np.int64(2015), np.int64(2020)]\n",
      "[ListNet Fold 5] Epoch   1 | train loss = 3.7328\n",
      "[ListNet Fold 5] Epoch  20 | train loss = 2.2684\n",
      "[ListNet Fold 5] Epoch  40 | train loss = 2.0308\n",
      "[ListNet Fold 5] Epoch  60 | train loss = 1.8146\n",
      "[ListNet Fold 5] Epoch  80 | train loss = 1.6749\n",
      "[ListNet Fold 5] Epoch 100 | train loss = 1.5791\n",
      "[ListNet Fold 5] Epoch 120 | train loss = 1.5079\n",
      "[ListNet Fold 5] Epoch 140 | train loss = 1.4491\n",
      "[ListNet Fold 5] Epoch 160 | train loss = 1.3921\n",
      "[ListNet Fold 5] Epoch 180 | train loss = 1.3443\n",
      "[ListNet Fold 5] Epoch 200 | train loss = 1.3029\n",
      "[ListNet Fold 5] Train pairwise acc = 0.678 | Test pairwise acc = 0.682\n",
      "\n",
      "=== ListNet 5-fold CV (season-wise) ===\n",
      "Fold 1: train = 0.697, test = 0.688\n",
      "Fold 2: train = 0.692, test = 0.647\n",
      "Fold 3: train = 0.695, test = 0.668\n",
      "Fold 4: train = 0.698, test = 0.686\n",
      "Fold 5: train = 0.678, test = 0.682\n",
      "Mean train acc: 0.692\n",
      "Mean  test acc: 0.674\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 7. Train and evaluate ListNet\n",
    "# ----------------------------\n",
    "listnet_train_accs, listnet_test_accs = run_kfold_for_loss(\n",
    "    folds=folds,\n",
    "    loss_fn=listnet_loss,\n",
    "    model_name=\"ListNet\",\n",
    "    n_epochs=200,\n",
    "    hidden_dim=64,\n",
    "    lr=1e-3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e95dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ListMLE | Fold 1 =====\n",
      "Train seasons: [np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2010), np.int64(2011), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2000), np.int64(2009), np.int64(2012), np.int64(2017), np.int64(2024)]\n",
      "[ListMLE Fold 1] Epoch   1 | train loss = 141.3167\n",
      "[ListMLE Fold 1] Epoch  20 | train loss = 130.7051\n",
      "[ListMLE Fold 1] Epoch  40 | train loss = 129.2607\n",
      "[ListMLE Fold 1] Epoch  60 | train loss = 128.2911\n",
      "[ListMLE Fold 1] Epoch  80 | train loss = 127.3813\n",
      "[ListMLE Fold 1] Epoch 100 | train loss = 126.5648\n",
      "[ListMLE Fold 1] Epoch 120 | train loss = 125.8590\n",
      "[ListMLE Fold 1] Epoch 140 | train loss = 125.1991\n",
      "[ListMLE Fold 1] Epoch 160 | train loss = 124.5510\n",
      "[ListMLE Fold 1] Epoch 180 | train loss = 123.9752\n",
      "[ListMLE Fold 1] Epoch 200 | train loss = 123.4435\n",
      "[ListMLE Fold 1] Train pairwise acc = 0.767 | Test pairwise acc = 0.680\n",
      "\n",
      "===== ListMLE | Fold 2 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2001), np.int64(2006), np.int64(2010), np.int64(2014), np.int64(2023)]\n",
      "[ListMLE Fold 2] Epoch   1 | train loss = 141.6313\n",
      "[ListMLE Fold 2] Epoch  20 | train loss = 130.5385\n",
      "[ListMLE Fold 2] Epoch  40 | train loss = 129.0894\n",
      "[ListMLE Fold 2] Epoch  60 | train loss = 127.9074\n",
      "[ListMLE Fold 2] Epoch  80 | train loss = 126.9042\n",
      "[ListMLE Fold 2] Epoch 100 | train loss = 126.0488\n",
      "[ListMLE Fold 2] Epoch 120 | train loss = 125.2743\n",
      "[ListMLE Fold 2] Epoch 140 | train loss = 124.5564\n",
      "[ListMLE Fold 2] Epoch 160 | train loss = 123.8668\n",
      "[ListMLE Fold 2] Epoch 180 | train loss = 123.2575\n",
      "[ListMLE Fold 2] Epoch 200 | train loss = 122.6758\n",
      "[ListMLE Fold 2] Train pairwise acc = 0.769 | Test pairwise acc = 0.688\n",
      "\n",
      "===== ListMLE | Fold 3 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2014), np.int64(2015), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2013), np.int64(2016)]\n",
      "[ListMLE Fold 3] Epoch   1 | train loss = 147.2793\n",
      "[ListMLE Fold 3] Epoch  20 | train loss = 135.2849\n",
      "[ListMLE Fold 3] Epoch  40 | train loss = 133.5416\n",
      "[ListMLE Fold 3] Epoch  60 | train loss = 132.2318\n",
      "[ListMLE Fold 3] Epoch  80 | train loss = 131.2171\n",
      "[ListMLE Fold 3] Epoch 100 | train loss = 130.3124\n",
      "[ListMLE Fold 3] Epoch 120 | train loss = 129.4529\n",
      "[ListMLE Fold 3] Epoch 140 | train loss = 128.6574\n",
      "[ListMLE Fold 3] Epoch 160 | train loss = 127.9514\n",
      "[ListMLE Fold 3] Epoch 180 | train loss = 127.2810\n",
      "[ListMLE Fold 3] Epoch 200 | train loss = 126.6749\n",
      "[ListMLE Fold 3] Train pairwise acc = 0.777 | Test pairwise acc = 0.654\n",
      "\n",
      "===== ListMLE | Fold 4 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2020), np.int64(2023), np.int64(2024)]\n",
      "Test  seasons: [np.int64(2018), np.int64(2019), np.int64(2021), np.int64(2022), np.int64(2025)]\n",
      "[ListMLE Fold 4] Epoch   1 | train loss = 139.2475\n",
      "[ListMLE Fold 4] Epoch  20 | train loss = 128.6130\n",
      "[ListMLE Fold 4] Epoch  40 | train loss = 127.2557\n",
      "[ListMLE Fold 4] Epoch  60 | train loss = 126.2618\n",
      "[ListMLE Fold 4] Epoch  80 | train loss = 125.4730\n",
      "[ListMLE Fold 4] Epoch 100 | train loss = 124.7935\n",
      "[ListMLE Fold 4] Epoch 120 | train loss = 124.1079\n",
      "[ListMLE Fold 4] Epoch 140 | train loss = 123.4472\n",
      "[ListMLE Fold 4] Epoch 160 | train loss = 122.8308\n",
      "[ListMLE Fold 4] Epoch 180 | train loss = 122.2270\n",
      "[ListMLE Fold 4] Epoch 200 | train loss = 121.6442\n",
      "[ListMLE Fold 4] Train pairwise acc = 0.763 | Test pairwise acc = 0.700\n",
      "\n",
      "===== ListMLE | Fold 5 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2009), np.int64(2010), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2007), np.int64(2008), np.int64(2011), np.int64(2015), np.int64(2020)]\n",
      "[ListMLE Fold 5] Epoch   1 | train loss = 143.5803\n",
      "[ListMLE Fold 5] Epoch  20 | train loss = 132.9911\n",
      "[ListMLE Fold 5] Epoch  40 | train loss = 131.3787\n",
      "[ListMLE Fold 5] Epoch  60 | train loss = 130.3164\n",
      "[ListMLE Fold 5] Epoch  80 | train loss = 129.4104\n",
      "[ListMLE Fold 5] Epoch 100 | train loss = 128.5849\n",
      "[ListMLE Fold 5] Epoch 120 | train loss = 127.8501\n",
      "[ListMLE Fold 5] Epoch 140 | train loss = 127.2079\n",
      "[ListMLE Fold 5] Epoch 160 | train loss = 126.5835\n",
      "[ListMLE Fold 5] Epoch 180 | train loss = 125.9662\n",
      "[ListMLE Fold 5] Epoch 200 | train loss = 125.2849\n",
      "[ListMLE Fold 5] Train pairwise acc = 0.762 | Test pairwise acc = 0.713\n",
      "\n",
      "=== ListMLE 5-fold CV (season-wise) ===\n",
      "Fold 1: train = 0.767, test = 0.680\n",
      "Fold 2: train = 0.769, test = 0.688\n",
      "Fold 3: train = 0.777, test = 0.654\n",
      "Fold 4: train = 0.763, test = 0.700\n",
      "Fold 5: train = 0.762, test = 0.713\n",
      "Mean train acc: 0.768\n",
      "Mean  test acc: 0.687\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 8. Train and evaluate ListMLE\n",
    "# ----------------------------\n",
    "listmle_train_accs, listmle_test_accs = run_kfold_for_loss(\n",
    "    folds=folds,\n",
    "    loss_fn=listmle_loss,\n",
    "    model_name=\"ListMLE\",\n",
    "    n_epochs=200,\n",
    "    hidden_dim=64,\n",
    "    lr=1e-3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e6745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "def build_lgb_data_for_fold(df, feature_cols, train_seasons, test_seasons):\n",
    "    \"\"\"\n",
    "    Build LightGBM ranking data (X, y, group) for a given fold.\n",
    "    Scaling is fit on TRAIN seasons only.\n",
    "    Returns:\n",
    "      X_train, y_train_rel, group_train,\n",
    "      X_test,  y_test_rel,  group_test,\n",
    "      feat_mean, feat_std\n",
    "    \"\"\"\n",
    "    df_train = df[df[\"SEASON\"].isin(train_seasons)].copy()\n",
    "    df_test  = df[df[\"SEASON\"].isin(test_seasons)].copy()\n",
    "\n",
    "    # ---- scaling (TRAIN only) ----\n",
    "    train_feats = df_train[feature_cols]\n",
    "    feat_mean = train_feats.mean()\n",
    "    feat_std  = train_feats.std().replace(0, 1.0)\n",
    "\n",
    "    def build_X_y_group(df_subset, seasons_subset):\n",
    "        dfs = []\n",
    "        ys = []\n",
    "        group = []\n",
    "        for season in seasons_subset:\n",
    "            g = df_subset[df_subset[\"SEASON\"] == season].copy()\n",
    "            if g.empty:\n",
    "                continue\n",
    "            g = g.sort_values(\"OVERALL_PICK\")  # lower pick = better\n",
    "            dfs.append(g)\n",
    "            ys.append(g[\"OVERALL_PICK\"].values.astype(float))\n",
    "            group.append(len(g))\n",
    "        if not dfs:\n",
    "            return np.empty((0, len(feature_cols))), np.array([]), []\n",
    "        df_cat = pd.concat(dfs, axis=0)\n",
    "        X = ((df_cat[feature_cols] - feat_mean) / feat_std).values\n",
    "        y = np.concatenate(ys, axis=0)\n",
    "        return X, y, group\n",
    "\n",
    "    X_train, y_train, group_train = build_X_y_group(df_train, train_seasons)\n",
    "    X_test,  y_test,  group_test  = build_X_y_group(df_test,  test_seasons)\n",
    "\n",
    "    # LightGBM expects \"higher is better\"\n",
    "    max_y = y_train.max()\n",
    "    y_train_rel = max_y - y_train   # pick 1 → big number, pick 60 → small number\n",
    "    y_test_rel  = max_y - y_test\n",
    "\n",
    "\n",
    "    return (\n",
    "        X_train, y_train_rel, group_train,\n",
    "        X_test,  y_test_rel,  group_test,\n",
    "        feat_mean, feat_std,\n",
    "    )\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "def run_lambdamart_cv(df, feature_cols, k_folds=5, random_state=42, num_boost_round=300):\n",
    "    \"\"\"\n",
    "    LambdaMART (LightGBM lambdarank) with season-wise K-fold CV.\n",
    "    Uses your existing `prepare_kfold_folds` to define folds.\n",
    "    Computes train & test metrics per fold and their averages.\n",
    "    \"\"\"\n",
    "    folds = prepare_kfold_folds(df, feature_cols, k_folds=k_folds, random_state=random_state)\n",
    "\n",
    "    base_params = {\n",
    "        \"objective\": \"lambdarank\",\n",
    "        \"metric\": \"ndcg\",\n",
    "        \"ndcg_at\": [5, 10, 20],\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"min_data_in_leaf\": 20,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"max_depth\": -1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "\n",
    "    train_pair_list = []\n",
    "    test_pair_list  = []\n",
    "    train_spear_list = []\n",
    "    test_spear_list  = []\n",
    "    train_kend_list  = []\n",
    "    test_kend_list   = []\n",
    "\n",
    "    for fold in folds:\n",
    "        fold_id = fold[\"fold_id\"]\n",
    "        train_seasons = fold[\"train_seasons\"]\n",
    "        test_seasons  = fold[\"test_seasons\"]\n",
    "\n",
    "        print(f\"\\n===== LambdaMART | Fold {fold_id} =====\")\n",
    "        print(\"Train seasons:\", train_seasons)\n",
    "        print(\"Test  seasons:\", test_seasons)\n",
    "\n",
    "        # ---- Rebuild raw train/test data for this fold ----\n",
    "        df_train = df[df[\"SEASON\"].isin(train_seasons)].copy()\n",
    "        df_test  = df[df[\"SEASON\"].isin(test_seasons)].copy()\n",
    "\n",
    "        # ---- Fit scaling on TRAIN only ----\n",
    "        feat_mean = df_train[feature_cols].mean()\n",
    "        feat_std  = df_train[feature_cols].std().replace(0, 1.0)\n",
    "\n",
    "        def build_group(df_slice, seasons):\n",
    "            dfs = []\n",
    "            ys = []\n",
    "            groups = []\n",
    "            for s in seasons:\n",
    "                g = df_slice[df_slice[\"SEASON\"] == s].copy()\n",
    "                if g.empty:\n",
    "                    continue\n",
    "                g = g.sort_values(\"OVERALL_PICK\")\n",
    "                dfs.append(g)\n",
    "                ys.append(g[\"OVERALL_PICK\"].values.astype(float))\n",
    "                groups.append(len(g))\n",
    "            if not dfs:\n",
    "                return np.empty((0, len(feature_cols))), np.array([]), []\n",
    "            df_cat = pd.concat(dfs, axis=0)\n",
    "            X = ((df_cat[feature_cols] - feat_mean) / feat_std).values\n",
    "            y = np.concatenate(ys, axis=0)\n",
    "            return X, y, groups\n",
    "\n",
    "        X_train, y_train, group_train = build_group(df_train, train_seasons)\n",
    "        X_test,  y_test,  group_test  = build_group(df_test,  test_seasons)\n",
    "\n",
    "        # ---- Map picks -> non-negative integer relevance ----\n",
    "        max_y = y_train.max()\n",
    "        y_train_rel = (max_y - y_train).astype(int)\n",
    "        y_test_rel  = (max_y - y_test).astype(int)\n",
    "\n",
    "        assert y_train_rel.min() >= 0\n",
    "        assert y_test_rel.min()  >= 0\n",
    "\n",
    "        # ---- label_gain long enough ----\n",
    "        max_label = int(max(y_train_rel.max(), y_test_rel.max()))\n",
    "        params = dict(base_params)\n",
    "        params[\"label_gain\"] = list(range(max_label + 1))\n",
    "\n",
    "        train_set = lgb.Dataset(X_train, label=y_train_rel, group=group_train)\n",
    "        valid_set = lgb.Dataset(X_test,  label=y_test_rel,  group=group_test, reference=train_set)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_set,\n",
    "            num_boost_round=num_boost_round,\n",
    "            valid_sets=[valid_set],\n",
    "            valid_names=[\"valid\"],\n",
    "        )\n",
    "\n",
    "        # ---- Evaluate on TRAIN + TEST seasons ----\n",
    "        train_pair, train_spear, train_kend = evaluate_lambdamart_fold(\n",
    "            model, df, feature_cols, feat_mean, feat_std, train_seasons\n",
    "        )\n",
    "        test_pair, test_spear, test_kend = evaluate_lambdamart_fold(\n",
    "            model, df, feature_cols, feat_mean, feat_std, test_seasons\n",
    "        )\n",
    "\n",
    "        print(f\"[Fold {fold_id}]\")\n",
    "        print(f\"  Train: Pairwise = {train_pair:.3f}, Spearman = {train_spear:.3f}, Kendall = {train_kend:.3f}\")\n",
    "        print(f\"  Test : Pairwise = {test_pair:.3f}, Spearman = {test_spear:.3f}, Kendall = {test_kend:.3f}\")\n",
    "\n",
    "        train_pair_list.append(train_pair)\n",
    "        test_pair_list.append(test_pair)\n",
    "        train_spear_list.append(train_spear)\n",
    "        test_spear_list.append(test_spear)\n",
    "        train_kend_list.append(train_kend)\n",
    "        test_kend_list.append(test_kend)\n",
    "\n",
    "    print(\"\\n=== LambdaMART K-fold CV (season-wise) ===\")\n",
    "    for i, (tr_p, te_p, tr_s, te_s, tr_k, te_k) in enumerate(\n",
    "        zip(train_pair_list, test_pair_list,\n",
    "            train_spear_list, test_spear_list,\n",
    "            train_kend_list, test_kend_list),\n",
    "        start=1,\n",
    "    ):\n",
    "        print(f\"Fold {i}: \"\n",
    "              f\"TrainPair = {tr_p:.3f}, TestPair = {te_p:.3f} | \"\n",
    "              f\"TrainSpearman = {tr_s:.3f}, TestSpearman = {te_s:.3f} | \"\n",
    "              f\"TrainKendall = {tr_k:.3f}, TestKendall = {te_k:.3f}\")\n",
    "\n",
    "    print(\"\\nMean Train pairwise:\", np.mean(train_pair_list))\n",
    "    print(\"Mean Test  pairwise:\", np.mean(test_pair_list))\n",
    "    print(\"Mean Train Spearman:\", np.mean(train_spear_list))\n",
    "    print(\"Mean Test  Spearman:\", np.mean(test_spear_list))\n",
    "    print(\"Mean Train Kendall :\", np.mean(train_kend_list))\n",
    "    print(\"Mean Test  Kendall :\", np.mean(test_kend_list))\n",
    "\n",
    "    return {\n",
    "        \"train_pair\":  train_pair_list,\n",
    "        \"test_pair\":   test_pair_list,\n",
    "        \"train_spear\": train_spear_list,\n",
    "        \"test_spear\":  test_spear_list,\n",
    "        \"train_kend\":  train_kend_list,\n",
    "        \"test_kend\":   test_kend_list,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_lambdamart_fold(model, df, feature_cols, feat_mean, feat_std, seasons):\n",
    "    \"\"\"\n",
    "    Evaluate LambdaMART model on given seasons:\n",
    "      - pairwise accuracy\n",
    "      - mean Spearman\n",
    "      - mean Kendall\n",
    "    \"\"\"\n",
    "    total_correct = 0\n",
    "    total_pairs = 0\n",
    "    spear_list = []\n",
    "    kend_list = []\n",
    "\n",
    "    for s in seasons:\n",
    "        g = df[df[\"SEASON\"] == s].copy()\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        g = g.sort_values(\"OVERALL_PICK\")\n",
    "        X = ((g[feature_cols] - feat_mean) / feat_std).values\n",
    "        true_pick = g[\"OVERALL_PICK\"].values.astype(float)\n",
    "\n",
    "        scores = model.predict(X)\n",
    "        n = len(true_pick)\n",
    "\n",
    "        # pairwise accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                total += 1\n",
    "                true_better = true_pick[i] < true_pick[j]   # smaller pick = better\n",
    "                pred_better = scores[i] > scores[j]         # higher score = better\n",
    "                if true_better == pred_better:\n",
    "                    correct += 1\n",
    "        total_correct += correct\n",
    "        total_pairs += total\n",
    "\n",
    "        # rank correlations (negate picks so higher is better)\n",
    "        spear, _ = spearmanr(-true_pick, scores)\n",
    "        kend, _ = kendalltau(-true_pick, scores)\n",
    "        spear_list.append(spear)\n",
    "        kend_list.append(kend)\n",
    "\n",
    "    pair_acc = total_correct / total_pairs if total_pairs > 0 else 0.0\n",
    "    mean_spear = float(np.nanmean(spear_list)) if spear_list else 0.0\n",
    "    mean_kend  = float(np.nanmean(kend_list))  if kend_list else 0.0\n",
    "\n",
    "    return pair_acc, mean_spear, mean_kend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9048ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LambdaMART | Fold 1 =====\n",
      "Train seasons: [np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2010), np.int64(2011), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2000), np.int64(2009), np.int64(2012), np.int64(2017), np.int64(2024)]\n",
      "[Fold 1]\n",
      "  Train: Pairwise = 0.906, Spearman = 0.938, Kendall = 0.824\n",
      "  Test : Pairwise = 0.687, Spearman = 0.544, Kendall = 0.379\n",
      "\n",
      "===== LambdaMART | Fold 2 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2001), np.int64(2006), np.int64(2010), np.int64(2014), np.int64(2023)]\n",
      "[Fold 2]\n",
      "  Train: Pairwise = 0.903, Spearman = 0.935, Kendall = 0.817\n",
      "  Test : Pairwise = 0.677, Spearman = 0.499, Kendall = 0.360\n",
      "\n",
      "===== LambdaMART | Fold 3 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2014), np.int64(2015), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2013), np.int64(2016)]\n",
      "[Fold 3]\n",
      "  Train: Pairwise = 0.899, Spearman = 0.930, Kendall = 0.809\n",
      "  Test : Pairwise = 0.676, Spearman = 0.491, Kendall = 0.351\n",
      "\n",
      "===== LambdaMART | Fold 4 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2020), np.int64(2023), np.int64(2024)]\n",
      "Test  seasons: [np.int64(2018), np.int64(2019), np.int64(2021), np.int64(2022), np.int64(2025)]\n",
      "[Fold 4]\n",
      "  Train: Pairwise = 0.904, Spearman = 0.936, Kendall = 0.816\n",
      "  Test : Pairwise = 0.692, Spearman = 0.529, Kendall = 0.376\n",
      "\n",
      "===== LambdaMART | Fold 5 =====\n",
      "Train seasons: [np.int64(2000), np.int64(2001), np.int64(2002), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2009), np.int64(2010), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "Test  seasons: [np.int64(2007), np.int64(2008), np.int64(2011), np.int64(2015), np.int64(2020)]\n",
      "[Fold 5]\n",
      "  Train: Pairwise = 0.900, Spearman = 0.932, Kendall = 0.812\n",
      "  Test : Pairwise = 0.728, Spearman = 0.622, Kendall = 0.461\n",
      "\n",
      "=== LambdaMART K-fold CV (season-wise) ===\n",
      "Fold 1: TrainPair = 0.906, TestPair = 0.687 | TrainSpearman = 0.938, TestSpearman = 0.544 | TrainKendall = 0.824, TestKendall = 0.379\n",
      "Fold 2: TrainPair = 0.903, TestPair = 0.677 | TrainSpearman = 0.935, TestSpearman = 0.499 | TrainKendall = 0.817, TestKendall = 0.360\n",
      "Fold 3: TrainPair = 0.899, TestPair = 0.676 | TrainSpearman = 0.930, TestSpearman = 0.491 | TrainKendall = 0.809, TestKendall = 0.351\n",
      "Fold 4: TrainPair = 0.904, TestPair = 0.692 | TrainSpearman = 0.936, TestSpearman = 0.529 | TrainKendall = 0.816, TestKendall = 0.376\n",
      "Fold 5: TrainPair = 0.900, TestPair = 0.728 | TrainSpearman = 0.932, TestSpearman = 0.622 | TrainKendall = 0.812, TestKendall = 0.461\n",
      "\n",
      "Mean Train pairwise: 0.9022569302439241\n",
      "Mean Test  pairwise: 0.6918810509048494\n",
      "Mean Train Spearman: 0.9344915808938239\n",
      "Mean Test  Spearman: 0.537014125567121\n",
      "Mean Train Kendall : 0.8155301529977041\n",
      "Mean Test  Kendall : 0.38550785184730135\n"
     ]
    }
   ],
   "source": [
    "drop_cols = [\"player_name\", \"OVERALL_PICK\", \"SEASON\"]\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "lambdamart_cv_results = run_lambdamart_cv(\n",
    "    df=df,\n",
    "    feature_cols=feature_cols,\n",
    "    k_folds=5,\n",
    "    random_state=42,\n",
    "    num_boost_round=300,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7b776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
