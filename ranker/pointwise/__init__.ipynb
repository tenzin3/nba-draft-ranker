{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6b223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Section 0: Imports\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7d2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 1: Metrics (Spearman + Pairwise Acc)\n",
    "# ============================================\n",
    "def _rankdata_average_ties(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    order = np.argsort(x, kind=\"mergesort\")\n",
    "    ranks = np.empty_like(order, dtype=float)\n",
    "    ranks[order] = np.arange(len(x), dtype=float)\n",
    "\n",
    "    sorted_x = x[order]\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        j = i\n",
    "        while j + 1 < len(x) and sorted_x[j + 1] == sorted_x[i]:\n",
    "            j += 1\n",
    "        if j > i:\n",
    "            avg = (ranks[order[i]] + ranks[order[j]]) / 2.0\n",
    "            ranks[order[i:j+1]] = avg\n",
    "        i = j + 1\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def spearman_corr(pred_scores: np.ndarray, true_picks: np.ndarray) -> float:\n",
    "    pred_scores = np.asarray(pred_scores)\n",
    "    true_picks = np.asarray(true_picks)\n",
    "    y_true = -true_picks.astype(float)  # higher is better\n",
    "\n",
    "    ra = _rankdata_average_ties(pred_scores)\n",
    "    rb = _rankdata_average_ties(y_true)\n",
    "\n",
    "    ra -= ra.mean()\n",
    "    rb -= rb.mean()\n",
    "    denom = np.sqrt((ra**2).sum() * (rb**2).sum())\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    return float((ra * rb).sum() / denom)\n",
    "\n",
    "\n",
    "def pairwise_accuracy(pred_scores: np.ndarray, true_picks: np.ndarray) -> float:\n",
    "    pred_scores = np.asarray(pred_scores)\n",
    "    true_picks = np.asarray(true_picks)\n",
    "    n = len(pred_scores)\n",
    "    if n < 2:\n",
    "        return np.nan\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            true = true_picks[i] < true_picks[j]   # earlier pick better\n",
    "            pred = pred_scores[i] > pred_scores[j] # higher score better\n",
    "            correct += int(true == pred)\n",
    "            total += 1\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def evaluate_season_df(season_df: pd.DataFrame, score_col=\"score\") -> dict:\n",
    "    return {\n",
    "        \"Spearman_rho\": spearman_corr(season_df[score_col].to_numpy(), season_df[\"OVERALL_PICK\"].to_numpy()),\n",
    "        \"PairwiseAcc\": pairwise_accuracy(season_df[score_col].to_numpy(), season_df[\"OVERALL_PICK\"].to_numpy()),\n",
    "        \"N\": len(season_df)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c62310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 2: Load CSV + Train/Test split\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import spearmanr, kendalltau  \n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load data\n",
    "# ----------------------------\n",
    "root_dir = Path.cwd().parent.parent\n",
    "dataset_path = root_dir / \"data\" / \"cleaned\" / \"college_drafted\" / \"college_drafted_selected_features.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df = df.dropna(subset=[\"SEASON\", \"OVERALL_PICK\"]).copy()\n",
    "df[\"SEASON\"] = df[\"SEASON\"].astype(int)\n",
    "df[\"OVERALL_PICK\"] = df[\"OVERALL_PICK\"].astype(int)\n",
    "\n",
    "train_df = df[(df[\"SEASON\"] >= 2000) & (df[\"SEASON\"] <= 2024)].copy()\n",
    "test_df  = df[df[\"SEASON\"] == 2025].copy()\n",
    "\n",
    "assert len(train_df) > 0, \"No training rows found for seasons 2000–2024.\"\n",
    "if len(test_df) == 0:\n",
    "    print(\"WARNING: No rows found for SEASON == 2025. Test evaluation will be skipped.\")\n",
    "\n",
    "exclude = {\"player_name\", \"SEASON\", \"OVERALL_PICK\"}\n",
    "feature_cols = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "assert len(feature_cols) > 0, \"No numeric feature columns found.\"\n",
    "\n",
    "# IMPORTANT: no imputation -> ensure no NaNs in features\n",
    "if train_df[feature_cols].isna().any().any() or (len(test_df) and test_df[feature_cols].isna().any().any()):\n",
    "    raise ValueError(\"Found NaNs in features, but you requested no imputation. Clean/remove missing values first.\")\n",
    "\n",
    "X_train = train_df[feature_cols].to_numpy()\n",
    "y_train = (-train_df[\"OVERALL_PICK\"].astype(float)).to_numpy()\n",
    "groups  = train_df[\"SEASON\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8b0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 3: Define MULTIPLE pointwise rankers\n",
    "# ============================================\n",
    "# All are pointwise: regress score -> sort within season\n",
    "models = {\n",
    "    \"Ridge\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", Ridge(alpha=1.0, random_state=42))\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", RandomForestRegressor(\n",
    "            n_estimators=500,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \"ExtraTrees\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", ExtraTreesRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=3,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \"HistGB\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", HistGradientBoostingRegressor(\n",
    "            loss=\"squared_error\",\n",
    "            learning_rate=0.06,\n",
    "            max_depth=6,\n",
    "            max_leaf_nodes=31,\n",
    "            min_samples_leaf=30,\n",
    "            l2_regularization=1e-2,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a883936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CV fold summary (macro over seasons) ===\n",
      "       model  fold  macro_Spearman_rho  macro_PairwiseAcc  n_val_rows  n_val_seasons\n",
      "       Ridge     1            0.557709           0.702465         241              5\n",
      "       Ridge     2            0.572391           0.700092         199              4\n",
      "       Ridge     3            0.580916           0.709515         238              5\n",
      "       Ridge     4            0.621252           0.718594         236              5\n",
      "       Ridge     5            0.561905           0.696475         233              5\n",
      "RandomForest     1            0.545813           0.692475         241              5\n",
      "RandomForest     2            0.552788           0.692790         199              4\n",
      "RandomForest     3            0.520963           0.685777         238              5\n",
      "RandomForest     4            0.609560           0.719342         236              5\n",
      "RandomForest     5            0.532450           0.685287         233              5\n",
      "  ExtraTrees     1            0.552725           0.695413         241              5\n",
      "  ExtraTrees     2            0.546684           0.691722         199              4\n",
      "  ExtraTrees     3            0.524920           0.687336         238              5\n",
      "  ExtraTrees     4            0.600614           0.712419         236              5\n",
      "  ExtraTrees     5            0.501744           0.672664         233              5\n",
      "      HistGB     1            0.554594           0.696489         241              5\n",
      "      HistGB     2            0.591409           0.713445         199              4\n",
      "      HistGB     3            0.525729           0.689683         238              5\n",
      "      HistGB     4            0.557648           0.696130         236              5\n",
      "      HistGB     5            0.532658           0.685184         233              5\n",
      "\n",
      "=== CV mean across folds (per model) ===\n",
      "              macro_Spearman_rho  macro_PairwiseAcc\n",
      "model                                              \n",
      "Ridge                   0.578835           0.705428\n",
      "HistGB                  0.552408           0.696186\n",
      "RandomForest            0.552315           0.695134\n",
      "ExtraTrees              0.545337           0.691911\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Section 4: Season-wise CV (GroupKFold) for all models\n",
    "# ==================================================\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for name, base_model in models.items():\n",
    "    fold_rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_train, y_train, groups=groups), start=1):\n",
    "        model = clone(base_model)\n",
    "        model.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "\n",
    "        va_scores = model.predict(X_train[va_idx])\n",
    "        va_df = train_df.iloc[va_idx].copy()\n",
    "        va_df[\"score\"] = va_scores\n",
    "\n",
    "        # per-season metrics, then macro avg across seasons\n",
    "        season_metrics = []\n",
    "        for season, g in va_df.groupby(\"SEASON\"):\n",
    "            m = evaluate_season_df(g, score_col=\"score\")\n",
    "            season_metrics.append(m)\n",
    "\n",
    "        season_metrics = pd.DataFrame(season_metrics)\n",
    "        fold_rows.append({\n",
    "            \"model\": name,\n",
    "            \"fold\": fold,\n",
    "            \"macro_Spearman_rho\": season_metrics[\"Spearman_rho\"].mean(),\n",
    "            \"macro_PairwiseAcc\": season_metrics[\"PairwiseAcc\"].mean(),\n",
    "            \"n_val_rows\": len(va_df),\n",
    "            \"n_val_seasons\": va_df[\"SEASON\"].nunique()\n",
    "        })\n",
    "\n",
    "    cv_results.append(pd.DataFrame(fold_rows))\n",
    "\n",
    "cv_summary = pd.concat(cv_results, ignore_index=True)\n",
    "\n",
    "print(\"\\n=== CV fold summary (macro over seasons) ===\")\n",
    "print(cv_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== CV mean across folds (per model) ===\")\n",
    "print(cv_summary.groupby(\"model\")[[\"macro_Spearman_rho\", \"macro_PairwiseAcc\"]].mean().sort_values(\"macro_Spearman_rho\", ascending=False).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5e5058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test results on SEASON == 2025 ===\n",
      "       model  Spearman_rho_2025  PairwiseAcc_2025  N_2025\n",
      "      HistGB           0.663417          0.736303      63\n",
      "RandomForest           0.641017          0.734255      63\n",
      "       Ridge           0.639817          0.727599      63\n",
      "  ExtraTrees           0.635927          0.735791      63\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 5: Train on 2000–2024, Test on 2025\n",
    "# ============================================\n",
    "if len(test_df) > 0:\n",
    "    X_test = test_df[feature_cols].to_numpy()\n",
    "\n",
    "    test_rows = []\n",
    "    for name, base_model in models.items():\n",
    "        model = clone(base_model)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        test_sc = model.predict(X_test)\n",
    "        tmp = test_df.copy()\n",
    "        tmp[\"score\"] = test_sc\n",
    "\n",
    "        # 2025 is a single season -> compute single-season metrics\n",
    "        m = evaluate_season_df(tmp, score_col=\"score\")\n",
    "        test_rows.append({\n",
    "            \"model\": name,\n",
    "            \"Spearman_rho_2025\": m[\"Spearman_rho\"],\n",
    "            \"PairwiseAcc_2025\": m[\"PairwiseAcc\"],\n",
    "            \"N_2025\": m[\"N\"]\n",
    "        })\n",
    "\n",
    "    test_summary = pd.DataFrame(test_rows).sort_values(\"Spearman_rho_2025\", ascending=False)\n",
    "    print(\"\\n=== Test results on SEASON == 2025 ===\")\n",
    "    print(test_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7526a7e",
   "metadata": {},
   "source": [
    "### Train MLP for pointwise Rankers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33119cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 1147  Train seasons: 24\n",
      "Test rows: 63  (SEASON==2025)\n",
      "Features: ['Totals_FG', 'Totals_FT', 'Totals_TRB', 'Totals_BLK', 'Totals_STL', 'Totals_TOV', 'Totals_PF', 'Shooting_FG%', 'MP', 'Age']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 2: Load CSV + Train/Test split\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import spearmanr, kendalltau  \n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load data\n",
    "# ----------------------------\n",
    "root_dir = Path.cwd().parent.parent\n",
    "dataset_path = root_dir / \"data\" / \"cleaned\" / \"college_drafted\" / \"college_drafted_selected_features.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.dropna(subset=[\"SEASON\", \"OVERALL_PICK\"]).copy()\n",
    "df[\"SEASON\"] = df[\"SEASON\"].astype(int)\n",
    "df[\"OVERALL_PICK\"] = df[\"OVERALL_PICK\"].astype(int)\n",
    "\n",
    "train_df = df[(df[\"SEASON\"] >= 2000) & (df[\"SEASON\"] <= 2024)].copy()\n",
    "test_df  = df[df[\"SEASON\"] == 2025].copy()\n",
    "\n",
    "exclude = {\"player_name\", \"SEASON\", \"OVERALL_PICK\"}\n",
    "feature_cols = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "# No imputation requested -> enforce no NaNs\n",
    "if train_df[feature_cols].isna().any().any() or (len(test_df) and test_df[feature_cols].isna().any().any()):\n",
    "    raise ValueError(\"Found NaNs in features. Clean/remove missing values since no imputation is used.\")\n",
    "\n",
    "X_train_raw = train_df[feature_cols].to_numpy().astype(np.float32)\n",
    "y_train = (-train_df[\"OVERALL_PICK\"].astype(float)).to_numpy().astype(np.float32)\n",
    "groups = train_df[\"SEASON\"].to_numpy()\n",
    "\n",
    "X_test_raw = test_df[feature_cols].to_numpy().astype(np.float32) if len(test_df) else None\n",
    "y_test_picks = test_df[\"OVERALL_PICK\"].to_numpy() if len(test_df) else None\n",
    "\n",
    "print(\"Train rows:\", len(train_df), \" Train seasons:\", train_df[\"SEASON\"].nunique())\n",
    "print(\"Test rows:\", len(test_df), \" (SEASON==2025)\")\n",
    "print(\"Features:\", feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b321249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 3: Define MLP pointwise ranker\n",
    "# ============================================\n",
    "class MLPPointwiseRanker(nn.Module):\n",
    "    def __init__(self, d_in: int, hidden=(128, 64), dropout=0.15):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = d_in\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))  # scalar score\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "def train_mlp(\n",
    "    X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "    X_va: np.ndarray, y_va: np.ndarray,\n",
    "    *,\n",
    "    hidden=(128, 64),\n",
    "    dropout=0.15,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=64,\n",
    "    max_epochs=300,\n",
    "    patience=25,\n",
    "    device=None,\n",
    "    seed=42\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = MLPPointwiseRanker(d_in=X_tr.shape[1], hidden=hidden, dropout=dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.SmoothL1Loss()  # Huber-like, robust to outliers\n",
    "\n",
    "    tr_ds = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
    "    va_ds = TensorDataset(torch.from_numpy(X_va), torch.from_numpy(y_va))\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    va_loader = DataLoader(va_ds, batch_size=256, shuffle=False, drop_last=False)\n",
    "\n",
    "    best_state = None\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        tr_losses = []\n",
    "        for xb, yb in tr_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        va_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                va_losses.append(loss.item())\n",
    "\n",
    "        val_loss = float(np.mean(va_losses))\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2484170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MLP CV fold summary (macro over seasons) ===\n",
      " fold  macro_Spearman_rho  macro_PairwiseAcc  n_val_rows  n_val_seasons\n",
      "    1            0.599673           0.716940         241              5\n",
      "    2            0.582296           0.711057         199              4\n",
      "    3            0.575365           0.710362         238              5\n",
      "    4            0.621091           0.717636         236              5\n",
      "    5            0.582319           0.705308         233              5\n",
      "\n",
      "=== MLP CV mean across folds ===\n",
      "macro_Spearman_rho    0.592149\n",
      "macro_PairwiseAcc     0.712261\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Section 4: Season-wise CV (GroupKFold) for MLP\n",
    "# ==================================================\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "cv_rows = []\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_train_raw, y_train, groups=groups), start=1):\n",
    "    X_tr_raw, X_va_raw = X_train_raw[tr_idx], X_train_raw[va_idx]\n",
    "    y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
    "\n",
    "    # Train-only standardization (no leakage)\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X_tr_raw).astype(np.float32)\n",
    "    X_va = scaler.transform(X_va_raw).astype(np.float32)\n",
    "\n",
    "    model = train_mlp(\n",
    "        X_tr, y_tr.astype(np.float32),\n",
    "        X_va, y_va.astype(np.float32),\n",
    "        hidden=(128, 64),\n",
    "        dropout=0.15,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        max_epochs=300,\n",
    "        patience=25,\n",
    "        device=device,\n",
    "        seed=42 + fold\n",
    "    )\n",
    "\n",
    "    # Predict on validation fold\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        va_scores = model(torch.from_numpy(X_va).to(device)).detach().cpu().numpy()\n",
    "\n",
    "    va_df = train_df.iloc[va_idx].copy()\n",
    "    va_df[\"score\"] = va_scores\n",
    "\n",
    "    season_metrics = []\n",
    "    for season, g in va_df.groupby(\"SEASON\"):\n",
    "        season_metrics.append(evaluate_season_df(g, score_col=\"score\"))\n",
    "\n",
    "    season_metrics = pd.DataFrame(season_metrics)\n",
    "\n",
    "    cv_rows.append({\n",
    "        \"fold\": fold,\n",
    "        \"macro_Spearman_rho\": season_metrics[\"Spearman_rho\"].mean(),\n",
    "        \"macro_PairwiseAcc\": season_metrics[\"PairwiseAcc\"].mean(),\n",
    "        \"n_val_rows\": len(va_df),\n",
    "        \"n_val_seasons\": va_df[\"SEASON\"].nunique()\n",
    "    })\n",
    "\n",
    "cv_summary = pd.DataFrame(cv_rows)\n",
    "print(\"\\n=== MLP CV fold summary (macro over seasons) ===\")\n",
    "print(cv_summary.to_string(index=False))\n",
    "print(\"\\n=== MLP CV mean across folds ===\")\n",
    "print(cv_summary[[\"macro_Spearman_rho\", \"macro_PairwiseAcc\"]].mean().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b2a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MLP Test results on SEASON == 2025 ===\n",
      "{'Spearman_rho_2025': 0.6464430940788876, 'PairwiseAcc_2025': 0.7260624679979518, 'N_2025': 63}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 5: Train on 2000–2024, Test on 2025\n",
    "# ============================================\n",
    "if len(test_df) > 0:\n",
    "    # Fit scaler on full train, transform train/test\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_full = scaler.fit_transform(X_train_raw).astype(np.float32)\n",
    "    X_te = scaler.transform(X_test_raw).astype(np.float32)\n",
    "\n",
    "    # Train with a small in-time validation split (last 2 seasons in train)\n",
    "    # to keep early stopping \"time-respecting\"\n",
    "    train_years = np.sort(train_df[\"SEASON\"].unique())\n",
    "    val_years = set(train_years[-2:])  # last 2 years of training window\n",
    "    is_val = train_df[\"SEASON\"].isin(val_years).to_numpy()\n",
    "\n",
    "    X_tr = X_tr_full[~is_val]\n",
    "    y_tr = y_train[~is_val]\n",
    "    X_va = X_tr_full[is_val]\n",
    "    y_va = y_train[is_val]\n",
    "\n",
    "    model = train_mlp(\n",
    "        X_tr, y_tr.astype(np.float32),\n",
    "        X_va, y_va.astype(np.float32),\n",
    "        hidden=(128, 64),\n",
    "        dropout=0.15,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        max_epochs=400,\n",
    "        patience=30,\n",
    "        device=device,\n",
    "        seed=2025\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_scores = model(torch.from_numpy(X_te).to(device)).detach().cpu().numpy()\n",
    "\n",
    "    tmp = test_df.copy()\n",
    "    tmp[\"score\"] = test_scores\n",
    "\n",
    "    m = evaluate_season_df(tmp, score_col=\"score\")\n",
    "    print(\"\\n=== MLP Test results on SEASON == 2025 ===\")\n",
    "    print({\n",
    "        \"Spearman_rho_2025\": m[\"Spearman_rho\"],\n",
    "        \"PairwiseAcc_2025\": m[\"PairwiseAcc\"],\n",
    "        \"N_2025\": m[\"N\"]\n",
    "    })\n",
    "else:\n",
    "    print(\"No 2025 rows found; skipping test evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d233a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
