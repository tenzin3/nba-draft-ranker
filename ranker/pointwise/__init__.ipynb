{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6b223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Section 0: Imports\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7d2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 1: Metrics (Spearman + Pairwise Acc)\n",
    "# ============================================\n",
    "def _rankdata_average_ties(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    order = np.argsort(x, kind=\"mergesort\")\n",
    "    ranks = np.empty_like(order, dtype=float)\n",
    "    ranks[order] = np.arange(len(x), dtype=float)\n",
    "\n",
    "    sorted_x = x[order]\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        j = i\n",
    "        while j + 1 < len(x) and sorted_x[j + 1] == sorted_x[i]:\n",
    "            j += 1\n",
    "        if j > i:\n",
    "            avg = (ranks[order[i]] + ranks[order[j]]) / 2.0\n",
    "            ranks[order[i:j+1]] = avg\n",
    "        i = j + 1\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def spearman_corr(pred_scores: np.ndarray, true_picks: np.ndarray) -> float:\n",
    "    pred_scores = np.asarray(pred_scores)\n",
    "    true_picks = np.asarray(true_picks)\n",
    "    y_true = -true_picks.astype(float)  # higher is better\n",
    "\n",
    "    ra = _rankdata_average_ties(pred_scores)\n",
    "    rb = _rankdata_average_ties(y_true)\n",
    "\n",
    "    ra -= ra.mean()\n",
    "    rb -= rb.mean()\n",
    "    denom = np.sqrt((ra**2).sum() * (rb**2).sum())\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    return float((ra * rb).sum() / denom)\n",
    "\n",
    "\n",
    "def pairwise_accuracy(pred_scores: np.ndarray, true_picks: np.ndarray) -> float:\n",
    "    pred_scores = np.asarray(pred_scores)\n",
    "    true_picks = np.asarray(true_picks)\n",
    "    n = len(pred_scores)\n",
    "    if n < 2:\n",
    "        return np.nan\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            true = true_picks[i] < true_picks[j]   # earlier pick better\n",
    "            pred = pred_scores[i] > pred_scores[j] # higher score better\n",
    "            correct += int(true == pred)\n",
    "            total += 1\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def evaluate_season_df(season_df: pd.DataFrame, score_col=\"score\") -> dict:\n",
    "    return {\n",
    "        \"Spearman_rho\": spearman_corr(season_df[score_col].to_numpy(), season_df[\"OVERALL_PICK\"].to_numpy()),\n",
    "        \"PairwiseAcc\": pairwise_accuracy(season_df[score_col].to_numpy(), season_df[\"OVERALL_PICK\"].to_numpy()),\n",
    "        \"N\": len(season_df)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c62310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 2: Load CSV + Train/Test split\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import spearmanr, kendalltau  \n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load data\n",
    "# ----------------------------\n",
    "root_dir = Path.cwd().parent.parent\n",
    "dataset_path = root_dir / \"data\" / \"cleaned\" / \"college_drafted\" / \"college_drafted_selected_features.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df = df.dropna(subset=[\"SEASON\", \"OVERALL_PICK\"]).copy()\n",
    "df[\"SEASON\"] = df[\"SEASON\"].astype(int)\n",
    "df[\"OVERALL_PICK\"] = df[\"OVERALL_PICK\"].astype(int)\n",
    "\n",
    "train_df = df[(df[\"SEASON\"] >= 2000) & (df[\"SEASON\"] <= 2024)].copy()\n",
    "test_df  = df[df[\"SEASON\"] == 2025].copy()\n",
    "\n",
    "assert len(train_df) > 0, \"No training rows found for seasons 2000–2024.\"\n",
    "if len(test_df) == 0:\n",
    "    print(\"WARNING: No rows found for SEASON == 2025. Test evaluation will be skipped.\")\n",
    "\n",
    "exclude = {\"player_name\", \"SEASON\", \"OVERALL_PICK\"}\n",
    "feature_cols = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "assert len(feature_cols) > 0, \"No numeric feature columns found.\"\n",
    "\n",
    "# IMPORTANT: no imputation -> ensure no NaNs in features\n",
    "if train_df[feature_cols].isna().any().any() or (len(test_df) and test_df[feature_cols].isna().any().any()):\n",
    "    raise ValueError(\"Found NaNs in features, but you requested no imputation. Clean/remove missing values first.\")\n",
    "\n",
    "X_train = train_df[feature_cols].to_numpy()\n",
    "y_train = (-train_df[\"OVERALL_PICK\"].astype(float)).to_numpy()\n",
    "groups  = train_df[\"SEASON\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8b0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 3: Define MULTIPLE pointwise rankers\n",
    "# ============================================\n",
    "# All are pointwise: regress score -> sort within season\n",
    "models = {\n",
    "    \"Ridge\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", Ridge(alpha=1.0, random_state=42))\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", RandomForestRegressor(\n",
    "            n_estimators=500,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \"ExtraTrees\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", ExtraTreesRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=3,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \"HistGB\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", HistGradientBoostingRegressor(\n",
    "            loss=\"squared_error\",\n",
    "            learning_rate=0.06,\n",
    "            max_depth=6,\n",
    "            max_leaf_nodes=31,\n",
    "            min_samples_leaf=30,\n",
    "            l2_regularization=1e-2,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a883936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CV fold summary (macro over seasons) ===\n",
      "       model  fold  macro_Spearman_rho  macro_PairwiseAcc  n_val_rows  n_val_seasons\n",
      "       Ridge     1            0.557709           0.702465         241              5\n",
      "       Ridge     2            0.572391           0.700092         199              4\n",
      "       Ridge     3            0.580916           0.709515         238              5\n",
      "       Ridge     4            0.621252           0.718594         236              5\n",
      "       Ridge     5            0.561905           0.696475         233              5\n",
      "RandomForest     1            0.545813           0.692475         241              5\n",
      "RandomForest     2            0.552788           0.692790         199              4\n",
      "RandomForest     3            0.520963           0.685777         238              5\n",
      "RandomForest     4            0.609560           0.719342         236              5\n",
      "RandomForest     5            0.532450           0.685287         233              5\n",
      "  ExtraTrees     1            0.552725           0.695413         241              5\n",
      "  ExtraTrees     2            0.546684           0.691722         199              4\n",
      "  ExtraTrees     3            0.524920           0.687336         238              5\n",
      "  ExtraTrees     4            0.600614           0.712419         236              5\n",
      "  ExtraTrees     5            0.501744           0.672664         233              5\n",
      "      HistGB     1            0.554594           0.696489         241              5\n",
      "      HistGB     2            0.591409           0.713445         199              4\n",
      "      HistGB     3            0.525729           0.689683         238              5\n",
      "      HistGB     4            0.557648           0.696130         236              5\n",
      "      HistGB     5            0.532658           0.685184         233              5\n",
      "\n",
      "=== CV mean across folds (per model) ===\n",
      "              macro_Spearman_rho  macro_PairwiseAcc\n",
      "model                                              \n",
      "Ridge                   0.578835           0.705428\n",
      "HistGB                  0.552408           0.696186\n",
      "RandomForest            0.552315           0.695134\n",
      "ExtraTrees              0.545337           0.691911\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Section 4: Season-wise CV (GroupKFold) for all models\n",
    "# ==================================================\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for name, base_model in models.items():\n",
    "    fold_rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_train, y_train, groups=groups), start=1):\n",
    "        model = clone(base_model)\n",
    "        model.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "\n",
    "        va_scores = model.predict(X_train[va_idx])\n",
    "        va_df = train_df.iloc[va_idx].copy()\n",
    "        va_df[\"score\"] = va_scores\n",
    "\n",
    "        # per-season metrics, then macro avg across seasons\n",
    "        season_metrics = []\n",
    "        for season, g in va_df.groupby(\"SEASON\"):\n",
    "            m = evaluate_season_df(g, score_col=\"score\")\n",
    "            season_metrics.append(m)\n",
    "\n",
    "        season_metrics = pd.DataFrame(season_metrics)\n",
    "        fold_rows.append({\n",
    "            \"model\": name,\n",
    "            \"fold\": fold,\n",
    "            \"macro_Spearman_rho\": season_metrics[\"Spearman_rho\"].mean(),\n",
    "            \"macro_PairwiseAcc\": season_metrics[\"PairwiseAcc\"].mean(),\n",
    "            \"n_val_rows\": len(va_df),\n",
    "            \"n_val_seasons\": va_df[\"SEASON\"].nunique()\n",
    "        })\n",
    "\n",
    "    cv_results.append(pd.DataFrame(fold_rows))\n",
    "\n",
    "cv_summary = pd.concat(cv_results, ignore_index=True)\n",
    "\n",
    "print(\"\\n=== CV fold summary (macro over seasons) ===\")\n",
    "print(cv_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== CV mean across folds (per model) ===\")\n",
    "print(cv_summary.groupby(\"model\")[[\"macro_Spearman_rho\", \"macro_PairwiseAcc\"]].mean().sort_values(\"macro_Spearman_rho\", ascending=False).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5e5058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/tenzin/Desktop/mycodes/nba-draft-ranker/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test results on SEASON == 2025 ===\n",
      "       model  Spearman_rho_2025  PairwiseAcc_2025  N_2025\n",
      "      HistGB           0.663417          0.736303      63\n",
      "RandomForest           0.641017          0.734255      63\n",
      "       Ridge           0.639817          0.727599      63\n",
      "  ExtraTrees           0.635927          0.735791      63\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 5: Train on 2000–2024, Test on 2025\n",
    "# ============================================\n",
    "if len(test_df) > 0:\n",
    "    X_test = test_df[feature_cols].to_numpy()\n",
    "\n",
    "    test_rows = []\n",
    "    for name, base_model in models.items():\n",
    "        model = clone(base_model)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        test_sc = model.predict(X_test)\n",
    "        tmp = test_df.copy()\n",
    "        tmp[\"score\"] = test_sc\n",
    "\n",
    "        # 2025 is a single season -> compute single-season metrics\n",
    "        m = evaluate_season_df(tmp, score_col=\"score\")\n",
    "        test_rows.append({\n",
    "            \"model\": name,\n",
    "            \"Spearman_rho_2025\": m[\"Spearman_rho\"],\n",
    "            \"PairwiseAcc_2025\": m[\"PairwiseAcc\"],\n",
    "            \"N_2025\": m[\"N\"]\n",
    "        })\n",
    "\n",
    "    test_summary = pd.DataFrame(test_rows).sort_values(\"Spearman_rho_2025\", ascending=False)\n",
    "    print(\"\\n=== Test results on SEASON == 2025 ===\")\n",
    "    print(test_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33119cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
