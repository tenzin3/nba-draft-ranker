{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bbe3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def _find_repo_root() -> Path:\n",
    "    here = Path.cwd().resolve()\n",
    "    for base in [here, *here.parents]:\n",
    "        if (base / 'ranker').is_dir():\n",
    "            return base\n",
    "    return here\n",
    "\n",
    "ROOT = _find_repo_root()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Pairwise / LTR helpers\n",
    "from ranker.pairwise import (\n",
    "    load_and_clean, pick_feature_columns, select_top_features,\n",
    "    ltr_fit_predict, pairwise_fit_predict, random_search_ltr, grid_search_ltr,\n",
    "    write_rankings_csv, evaluate_spearman_by_season, RANDOM_STATE, DEFAULT_DATA_PATHS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a5ce17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clean] Deduped (season, overall_pick): removed 113 rows (had 113 duplicates).\n",
      "/Users/young/code/pa_project/nba-draft-ranker/outputs/college_stats.csv (1097, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_name</th>\n",
       "      <th>overall_pick</th>\n",
       "      <th>season</th>\n",
       "      <th>totals_fg</th>\n",
       "      <th>totals_ft</th>\n",
       "      <th>totals_trb</th>\n",
       "      <th>totals_blk</th>\n",
       "      <th>totals_stl</th>\n",
       "      <th>totals_tov</th>\n",
       "      <th>totals_pf</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>position_group_UNK</th>\n",
       "      <th>position_group_nan</th>\n",
       "      <th>totals_fg_per_min</th>\n",
       "      <th>totals_ft_per_min</th>\n",
       "      <th>totals_trb_per_min</th>\n",
       "      <th>totals_blk_per_min</th>\n",
       "      <th>totals_stl_per_min</th>\n",
       "      <th>totals_tov_per_min</th>\n",
       "      <th>totals_pf_per_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kemba Walker</td>\n",
       "      <td>9</td>\n",
       "      <td>2011</td>\n",
       "      <td>316</td>\n",
       "      <td>258</td>\n",
       "      <td>223</td>\n",
       "      <td>7.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.204796</td>\n",
       "      <td>0.167207</td>\n",
       "      <td>0.144524</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.049903</td>\n",
       "      <td>0.060272</td>\n",
       "      <td>0.036293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Devonte' Graham</td>\n",
       "      <td>34</td>\n",
       "      <td>2018</td>\n",
       "      <td>199</td>\n",
       "      <td>167</td>\n",
       "      <td>157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.135007</td>\n",
       "      <td>0.113297</td>\n",
       "      <td>0.106513</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.042062</td>\n",
       "      <td>0.073948</td>\n",
       "      <td>0.037992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jonny Flynn</td>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "      <td>219</td>\n",
       "      <td>180</td>\n",
       "      <td>104</td>\n",
       "      <td>6.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.154443</td>\n",
       "      <td>0.126939</td>\n",
       "      <td>0.073343</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.038082</td>\n",
       "      <td>0.090973</td>\n",
       "      <td>0.038787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D.J. Augustin</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>242</td>\n",
       "      <td>173</td>\n",
       "      <td>112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.170783</td>\n",
       "      <td>0.122089</td>\n",
       "      <td>0.079040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.043049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michael Carter-Williams</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>155</td>\n",
       "      <td>129</td>\n",
       "      <td>199</td>\n",
       "      <td>20.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.110007</td>\n",
       "      <td>0.091554</td>\n",
       "      <td>0.141235</td>\n",
       "      <td>0.014194</td>\n",
       "      <td>0.077360</td>\n",
       "      <td>0.097942</td>\n",
       "      <td>0.065295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               player_name  overall_pick  season  totals_fg  totals_ft  \\\n",
       "0             Kemba Walker             9    2011        316        258   \n",
       "1          Devonte' Graham            34    2018        199        167   \n",
       "2              Jonny Flynn             6    2009        219        180   \n",
       "3            D.J. Augustin             9    2008        242        173   \n",
       "4  Michael Carter-Williams            11    2013        155        129   \n",
       "\n",
       "   totals_trb  totals_blk  totals_stl  totals_tov  totals_pf  ...   age  \\\n",
       "0         223         7.0        77.0        93.0       56.0  ...  20.0   \n",
       "1         157         2.0        62.0       109.0       56.0  ...  22.0   \n",
       "2         104         6.0        54.0       129.0       55.0  ...  19.0   \n",
       "3         112         0.0        47.0       105.0       61.0  ...  20.0   \n",
       "4         199        20.0       109.0       138.0       92.0  ...  21.0   \n",
       "\n",
       "   position_group_UNK  position_group_nan  totals_fg_per_min  \\\n",
       "0                True               False           0.204796   \n",
       "1                True               False           0.135007   \n",
       "2                True               False           0.154443   \n",
       "3                True               False           0.170783   \n",
       "4                True               False           0.110007   \n",
       "\n",
       "   totals_ft_per_min  totals_trb_per_min  totals_blk_per_min  \\\n",
       "0           0.167207            0.144524            0.004537   \n",
       "1           0.113297            0.106513            0.001357   \n",
       "2           0.126939            0.073343            0.004231   \n",
       "3           0.122089            0.079040            0.000000   \n",
       "4           0.091554            0.141235            0.014194   \n",
       "\n",
       "   totals_stl_per_min  totals_tov_per_min  totals_pf_per_min  \n",
       "0            0.049903            0.060272           0.036293  \n",
       "1            0.042062            0.073948           0.037992  \n",
       "2            0.038082            0.090973           0.038787  \n",
       "3            0.033169            0.074100           0.043049  \n",
       "4            0.077360            0.097942           0.065295  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Load data\n",
    "# ----------------------------\n",
    "data_path = DEFAULT_DATA_PATHS[0]\n",
    "df = load_and_clean(data_path)\n",
    "print(data_path, df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf9479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base features (19): ['shooting_fg%', 'mp', 'age', 'totals_fg_per_min', 'totals_ft_per_min', 'totals_trb_per_min', 'totals_blk_per_min', 'totals_stl_per_min', 'totals_tov_per_min', 'totals_pf_per_min', 'totals_fg', 'totals_ft', 'totals_trb', 'totals_blk', 'totals_stl', 'totals_tov', 'totals_pf', 'position_group_UNK', 'position_group_nan']\n",
      "Selected top 12 features: ['age', 'shooting_fg%', 'totals_blk_per_min', 'totals_trb', 'totals_blk', 'totals_ft_per_min', 'totals_trb_per_min', 'totals_fg_per_min', 'totals_fg', 'totals_ft', 'totals_stl', 'totals_stl_per_min']\n",
      "Top scores:\n",
      "  age                  spearman=-0.473 mi=0.1398 combined=0.973\n",
      "  shooting_fg%         spearman=+0.140 mi=0.0514 combined=0.324\n",
      "  totals_blk_per_min   spearman=+0.153 mi=0.0436 combined=0.309\n",
      "  totals_trb           spearman=+0.115 mi=0.0499 combined=0.293\n",
      "  totals_blk           spearman=+0.174 mi=0.0116 combined=0.216\n",
      "  totals_ft_per_min    spearman=+0.133 mi=0.0193 combined=0.202\n",
      "  totals_trb_per_min   spearman=+0.088 mi=0.0313 combined=0.200\n",
      "  totals_fg_per_min    spearman=+0.188 mi=0.0000 combined=0.188\n",
      "  totals_fg            spearman=+0.151 mi=0.0044 combined=0.167\n",
      "  totals_ft            spearman=+0.130 mi=0.0000 combined=0.130\n",
      "  totals_stl           spearman=+0.091 mi=0.0074 combined=0.117\n",
      "  totals_stl_per_min   spearman=+0.094 mi=0.0000 combined=0.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/code/pa_project/nba-draft-ranker/ranker/pairwise/pairwise_rank.py:191: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  sp = float(spearmanr(y[mask], s[mask]).correlation)\n",
      "/Users/young/code/pa_project/nba-draft-ranker/ranker/pairwise/pairwise_rank.py:191: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  sp = float(spearmanr(y[mask], s[mask]).correlation)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2. Feature selection (Spearman + mutual info)\n",
    "# ----------------------------\n",
    "feature_cols, dropped_cols = pick_feature_columns(df)\n",
    "print(f'Base features ({len(feature_cols)}):', feature_cols)\n",
    "top_k = min(12, len(feature_cols))\n",
    "feature_cols, scores = select_top_features(df, feature_cols, top_k)\n",
    "print(f'Selected top {len(feature_cols)} features: {feature_cols}')\n",
    "if scores:\n",
    "    print('Top scores:')\n",
    "    for col, sp, mi, comb in scores[:top_k]:\n",
    "        print(f\"  {col:20s} spearman={sp:+.3f} mi={mi:.4f} combined={comb:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff7a4a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train seasons <= 2021 (holdout=4): (925, 22) | Test: (172, 22)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 3. Train/test split by season (match CLI defaults)\n",
    "# ----------------------------\n",
    "holdout_seasons = 4  # same as CLI default\n",
    "seasons = np.sort(df['season'].unique())\n",
    "if len(seasons) <= holdout_seasons:\n",
    "    train_last = int(seasons.max())\n",
    "else:\n",
    "    train_last = int(seasons[-(holdout_seasons + 1)])\n",
    "train_df = df[df['season'] <= train_last].reset_index(drop=True)\n",
    "test_df = df[df['season'] > train_last].reset_index(drop=True)\n",
    "print(f'Train seasons <= {train_last} (holdout={holdout_seasons}):', train_df.shape, '| Test:', test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6203e11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip K-fold; set do_kfold_cv=True to run.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 3b. Season-level K-fold (optional)\n",
    "# ----------------------------\n",
    "do_kfold_cv = False  # set True to run K-fold across seasons\n",
    "k_folds = min(5, len(np.unique(df['season'])))\n",
    "\n",
    "if do_kfold_cv and k_folds >= 2:\n",
    "    from sklearn.model_selection import KFold\n",
    "    seasons_all = np.sort(df['season'].unique())\n",
    "    kf = KFold(n_splits=k_folds, shuffle=False)\n",
    "    cv_scores = []\n",
    "    for fold, (tr_idx, te_idx) in enumerate(kf.split(seasons_all), 1):\n",
    "        train_seasons = seasons_all[tr_idx]\n",
    "        test_seasons = seasons_all[te_idx]\n",
    "        tr_df = df[df['season'].isin(train_seasons)].reset_index(drop=True)\n",
    "        te_df = df[df['season'].isin(test_seasons)].reset_index(drop=True)\n",
    "\n",
    "        ensemble_cfgs_cv = [\n",
    "            {'seed': RANDOM_STATE, 'num_leaves':31, 'learning_rate':0.05, 'num_boost_round':3000, 'min_child_samples':20},\n",
    "            {'seed': RANDOM_STATE+101, 'num_leaves':47, 'learning_rate':0.04, 'num_boost_round':3500, 'min_child_samples':25},\n",
    "            {'seed': RANDOM_STATE+202, 'num_leaves':63, 'learning_rate':0.035, 'num_boost_round':4000, 'min_child_samples':30},\n",
    "        ]\n",
    "\n",
    "        scores_cv, sorted_cv = ltr_fit_predict(\n",
    "            tr_df, te_df, feature_cols, season_zscore=True, val_last_k=2, ensemble_cfgs=ensemble_cfgs_cv\n",
    "        )\n",
    "        corrs_cv = evaluate_spearman_by_season(sorted_cv, scores_cv)\n",
    "        vals_cv = [c for _, c in corrs_cv if not np.isnan(c)]\n",
    "        mean_cv = float(np.mean(vals_cv)) if vals_cv else float('nan')\n",
    "        cv_scores.append(mean_cv)\n",
    "        print(f\"Fold {fold}: train seasons {train_seasons} | test seasons {test_seasons} | LTR avg Spearman={mean_cv:.3f}\")\n",
    "\n",
    "    if cv_scores:\n",
    "        print('K-fold mean LTR Spearman:', float(np.nanmean(cv_scores)))\n",
    "else:\n",
    "    print('Skip K-fold; set do_kfold_cv=True to run.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a6c30c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\tvalid_0's mean_spearman: 0.409119\n",
      "[500]\tvalid_0's mean_spearman: 0.393156\n",
      "[250]\tvalid_0's mean_spearman: 0.395294\n",
      "[250]\tvalid_0's mean_spearman: 0.425559\n",
      "LTR Spearman by season: [(2022, 0.5326148610323312), (2023, 0.4803689064558629), (2024, 0.583015963049996), (2025, 0.7946239806704923)]\n",
      "LTR avg Spearman: 0.5976559278021706\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4. LTR (LightGBM LambdaRank)\n",
    "# ----------------------------\n",
    "use_random_search = False  # set True to try random search\n",
    "use_grid_search = False    # set True to try grid search\n",
    "season_zscore = True       # recommended for this data\n",
    "val_last_k = 2             # same as CLI default\n",
    "\n",
    "ensemble_cfgs = [\n",
    "    {'seed': RANDOM_STATE, 'num_leaves':31, 'learning_rate':0.05, 'num_boost_round':3000, 'min_child_samples':20},\n",
    "    {'seed': RANDOM_STATE+101, 'num_leaves':47, 'learning_rate':0.04, 'num_boost_round':3500, 'min_child_samples':25},\n",
    "    {'seed': RANDOM_STATE+202, 'num_leaves':63, 'learning_rate':0.035, 'num_boost_round':4000, 'min_child_samples':30},\n",
    "]\n",
    "\n",
    "if use_random_search:\n",
    "    best_cfg, best_score, _ = random_search_ltr(\n",
    "        train_df, feature_cols, season_zscore=season_zscore, val_last_k=val_last_k, n_trials=12\n",
    "    )\n",
    "    seeds = [RANDOM_STATE, RANDOM_STATE+101, RANDOM_STATE+202]\n",
    "    ensemble_cfgs = [{**best_cfg, 'seed': s} for s in seeds]\n",
    "    print('[Random search] best val spearman:', best_score)\n",
    "elif use_grid_search:\n",
    "    best_cfg, best_score, _ = grid_search_ltr(\n",
    "        train_df, feature_cols, season_zscore=season_zscore, val_last_k=val_last_k\n",
    "    )\n",
    "    seeds = [RANDOM_STATE, RANDOM_STATE+101, RANDOM_STATE+202]\n",
    "    ensemble_cfgs = [{**best_cfg, 'seed': s} for s in seeds]\n",
    "    print('[Grid search] best val spearman:', best_score)\n",
    "\n",
    "ltr_scores, ltr_sorted = ltr_fit_predict(\n",
    "    train_df, test_df, feature_cols, season_zscore=season_zscore, val_last_k=val_last_k, ensemble_cfgs=ensemble_cfgs\n",
    ")\n",
    "ltr_corrs = evaluate_spearman_by_season(ltr_sorted, ltr_scores)\n",
    "ltr_vals = [c for _, c in ltr_corrs if not np.isnan(c)]\n",
    "print('LTR Spearman by season:', ltr_corrs)\n",
    "if ltr_vals:\n",
    "    print('LTR avg Spearman:', float(np.mean(ltr_vals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]      log_reg  logloss=0.4363\n",
      "[CV]      sgd_log  logloss=0.4388\n",
      "[CV]          hgb  logloss=0.4451\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5. Pairwise ensemble baseline\n",
    "# ----------------------------\n",
    "pw_scores, pw_sorted = pairwise_fit_predict(\n",
    "    train_df, test_df, feature_cols, within_position_pairs=False, max_pairs_per_season=30000, n_splits=3\n",
    ")\n",
    "pw_corrs = evaluate_spearman_by_season(pw_sorted, pw_scores)\n",
    "pw_vals = [c for _, c in pw_corrs if not np.isnan(c)]\n",
    "print('Pairwise Spearman by season:', pw_corrs)\n",
    "if pw_vals:\n",
    "    print('Pairwise avg Spearman:', float(np.mean(pw_vals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d77d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra_trees_reg: avg Spearman=0.640 | per-season=[(2022, 0.5758852605137347), (2023, 0.5544137022397891), (2024, 0.6207762742079249), (2025, 0.8074599818785864)]\n",
      "hgb_reg: avg Spearman=0.576 | per-season=[(2022, 0.5190017016449234), (2023, 0.5272727272727272), (2024, 0.4977716554574184), (2025, 0.7585321655089098)]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5b. Additional ML baselines (regression on pick)\n",
    "# ----------------------------\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "ml_models = {}\n",
    "\n",
    "train_sorted_ml = train_df.sort_values(['season','overall_pick']).reset_index(drop=True)\n",
    "test_sorted_ml = test_df.sort_values(['season','overall_pick']).reset_index(drop=True)\n",
    "X_train_ml = train_sorted_ml[feature_cols]\n",
    "X_test_ml = test_sorted_ml[feature_cols]\n",
    "y_train_ml = -train_sorted_ml['overall_pick'].to_numpy()  # higher is better\n",
    "\n",
    "imp = SimpleImputer()\n",
    "scaler = StandardScaler()\n",
    "Xtr = scaler.fit_transform(imp.fit_transform(X_train_ml))\n",
    "Xte = scaler.transform(imp.transform(X_test_ml))\n",
    "\n",
    "for name, model in [\n",
    "    ('extra_trees_reg', ExtraTreesRegressor(\n",
    "        n_estimators=800, max_depth=None, min_samples_leaf=4, max_features='sqrt',\n",
    "        n_jobs=-1, random_state=RANDOM_STATE\n",
    "    )),\n",
    "    ('hgb_reg', HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05, max_depth=6, max_iter=600, l2_regularization=1e-2, random_state=RANDOM_STATE\n",
    "    )),\n",
    "]:\n",
    "    model.fit(Xtr, y_train_ml)\n",
    "    preds = model.predict(Xte)\n",
    "    corrs = evaluate_spearman_by_season(test_sorted_ml, preds)\n",
    "    ml_models[name] = (preds, corrs)\n",
    "    vals = [c for _, c in corrs if not np.isnan(c)]\n",
    "    mean_val = float(np.nanmean(vals)) if vals else float('nan')\n",
    "    print(f\"{name}: avg Spearman={mean_val:.3f} | per-season={corrs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d14baf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             method  avg_spearman\n",
      "13      hybrid_0.90      0.642998\n",
      "2   extra_trees_reg      0.639634\n",
      "12      hybrid_0.80      0.632280\n",
      "1          pairwise      0.630456\n",
      "14      hybrid_1.00      0.630456\n",
      "10      hybrid_0.60      0.622723\n",
      "11      hybrid_0.70      0.619138\n",
      "7       hybrid_0.30      0.610204\n",
      "9       hybrid_0.50      0.609911\n",
      "8       hybrid_0.40      0.609342\n",
      "6       hybrid_0.20      0.602204\n",
      "5       hybrid_0.10      0.599491\n",
      "0               ltr      0.597656\n",
      "4       hybrid_0.00      0.597656\n",
      "3           hgb_reg      0.575645\n",
      "Best method: {'method': 'hybrid_0.90', 'avg_spearman': np.float64(0.6429975480469944)}\n",
      "Best hybrid alpha: 0.90, avg Spearman=0.643\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 6. Model comparison table\n",
    "# ----------------------------\n",
    "results = []\n",
    "\n",
    "def _avg_corr(corrs):\n",
    "    vals = [c for _, c in corrs if not np.isnan(c)]\n",
    "    return float(np.nanmean(vals)) if vals else float('nan')\n",
    "\n",
    "if 'ltr_scores' in locals():\n",
    "    ltr_mean = _avg_corr(ltr_corrs)\n",
    "    results.append({'method': 'ltr', 'avg_spearman': ltr_mean})\n",
    "\n",
    "if 'pw_scores' in locals():\n",
    "    pw_mean = _avg_corr(pw_corrs)\n",
    "    results.append({'method': 'pairwise', 'avg_spearman': pw_mean})\n",
    "\n",
    "if 'ml_models' in locals():\n",
    "    for name, (preds, corrs) in ml_models.items():\n",
    "        results.append({'method': name, 'avg_spearman': _avg_corr(corrs)})\n",
    "\n",
    "hybrid_combo = None\n",
    "best_alpha = None\n",
    "best_mean = float('-inf')\n",
    "if 'ltr_scores' in locals() and 'pw_scores' in locals():\n",
    "    alphas = np.linspace(0.0, 1.0, 11)\n",
    "    for a in alphas:\n",
    "        hybrid_scores = a * pw_scores + (1.0 - a) * ltr_scores\n",
    "        hybrid_corrs = evaluate_spearman_by_season(pw_sorted, hybrid_scores)\n",
    "        m = _avg_corr(hybrid_corrs)\n",
    "        results.append({'method': f'hybrid_{a:.2f}', 'avg_spearman': m})\n",
    "        if not np.isnan(m) and m > best_mean:\n",
    "            best_mean = m\n",
    "            best_alpha = a\n",
    "            hybrid_combo = (hybrid_scores, hybrid_corrs)\n",
    "\n",
    "results_df = pd.DataFrame(results) if results else pd.DataFrame(columns=['method','avg_spearman'])\n",
    "print(results_df.sort_values('avg_spearman', ascending=False))\n",
    "if not results_df.empty:\n",
    "    best_method = results_df.sort_values('avg_spearman', ascending=False).iloc[0]\n",
    "    print('Best method:', dict(best_method))\n",
    "    if best_method['method'].startswith('hybrid') and best_alpha is not None:\n",
    "        print(f\"Best hybrid alpha: {best_alpha:.2f}, avg Spearman={best_mean:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702ba34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b193c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7. Choose final scores & write CSV\n",
    "# ----------------------------\n",
    "if 'best_method' in locals():\n",
    "    method_name = best_method['method']\n",
    "    if method_name == 'ltr' and 'ltr_scores' in locals():\n",
    "        final_scores = ltr_scores\n",
    "        final_df = ltr_sorted\n",
    "    elif method_name.startswith('hybrid') and 'hybrid_combo' in locals() and hybrid_combo is not None:\n",
    "        final_scores, _ = hybrid_combo\n",
    "        final_df = pw_sorted\n",
    "    elif 'ml_models' in locals() and method_name in ml_models:\n",
    "        final_scores, _ = ml_models[method_name]\n",
    "        final_df = test_sorted_ml\n",
    "    else:\n",
    "        final_scores = pw_scores\n",
    "        final_df = pw_sorted\n",
    "else:\n",
    "    final_scores = pw_scores  # fallback\n",
    "    final_df = pw_sorted\n",
    "\n",
    "write_rankings_csv(Path('../outputs/pairwise_rankings.csv'), final_df, final_scores)\n",
    "final_df.assign(pred_score=final_scores).head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
